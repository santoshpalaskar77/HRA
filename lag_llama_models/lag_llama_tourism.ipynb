{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import hts\n",
    "from hts.hierarchy import HierarchyTree\n",
    "\n",
    "from  forecasting_functions import probabilistic_forecasting\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.models import TCNModel,StatsForecastAutoARIMA,NHiTSModel\n",
    "import scipy.stats as ss\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import properscoring as ps\n",
    "\n",
    "# Fitting ARIMA model manually \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pl follow steps from: https://github.com/time-series-foundation-models/lag-llama/blob/main/README.md \\nto set the working directory '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' pl follow steps from: https://github.com/time-series-foundation-models/lag-llama/blob/main/README.md \n",
    "to set the working directory '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/tourism.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/tourism.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     test_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter the test length as an integer: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/hbpf_llama/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hbpf_llama/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/hbpf_llama/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hbpf_llama/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/hbpf_llama/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/tourism.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/tourism.csv\")\n",
    "\n",
    "try:\n",
    "    test_len = int(input(\"Please enter the test length as an integer: \"))\n",
    "    print(f\"Test length is set to: {test_len}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a valid integer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /lag-llama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir  lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
    "\n",
    "        # linear positional encoding scaling\n",
    "        rope_scaling={\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        },\n",
    "\n",
    "        batch_size=batch_size,\n",
    "        num_parallel_samples=num_samples,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
    "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Data consist of the information of all the tourists who visited Australia for various purposes between January 1998 to October 2006\n",
    "      Data is available every quarter. Data is mainly disaggregated based on four measure purposes of travel: Holiday (Hol), Vis-\n",
    "iting Friends and Relatives (VFR), Business (Bus), and Other (Oth). There are a total\n",
    "of four levels in the hierarchy. The total number of the series in the dataset is 89.\n",
    "\n",
    "\n",
    "Reference \n",
    "---------------\n",
    "\n",
    "1. Kamarthi, Harshavardhan, et al. \"PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series.\n",
    "\" arXiv preprint arXiv:2206.07940 (2022).\n",
    "\n",
    "      \n",
    "      '''\n",
    "\n",
    "\n",
    "df.rename(columns = {'Total':'total', 'Hol':'hol','VFR':'vfr','Bus':'bus','Oth':'oth'}, inplace = True)\n",
    "start = pd.to_datetime('1998-01-01')\n",
    "rng = pd.date_range(start, periods=36, freq='3M')\n",
    "index = pd.Index(rng)\n",
    "df1 = df\n",
    "df1 =df1.set_index(index)\n",
    "df1.index.name = 'date'\n",
    "\n",
    "freq = 'Q'\n",
    "\n",
    "# %%\n",
    "# Here we set the length of the train, test and validation set \n",
    "# We can generate the time series of desired size using following parameters\n",
    "#there are total 36 data points in each time series \n",
    "total_len = len(df1)\n",
    "train_len = total_len-test_len\n",
    "split_val = train_len\n",
    "\n",
    "purpose = np.array(df1.columns[1:5])\n",
    "purpose_r = list(range(1,5))\n",
    "states = np.array(df1.columns[5:33])\n",
    "states_r = list(range(5,33))\n",
    "city = np.array(df1.columns[33:len(df1.columns)])\n",
    "city_r = list(range(33,len(df1.columns)))\n",
    "\n",
    "level0 = [0]\n",
    "levels = [level0, purpose_r,states_r,city_r]\n",
    "bottom_level = city_r\n",
    "levels1 = [purpose,states,city]\n",
    "\n",
    "agg_length= 33\n",
    "\n",
    "# Defining hierarchy\n",
    "\n",
    "total = {'total': list(purpose)}\n",
    "purpose_h  = {k: [v for v in states  if v.endswith(k)] for k in purpose}\n",
    "state_h =  {k: [v for v in city  if v.startswith(k)] for k in states}\n",
    "\n",
    "hierarchy ={**total,**purpose_h,**state_h}  #state_h\n",
    "\n",
    "aac = list(hierarchy.keys())   # aggregated series \n",
    "aab= list(hierarchy.values())  # child nodes of each aggregated series \n",
    "\n",
    "\n",
    "'''Defining aggregation structure that will be needed while geeting the aggregayed \n",
    "forecast using child node forecasts'''\n",
    "agg_structure1 = []\n",
    "for i in range(len(aac)):\n",
    "    for j in range(len(levels1)):\n",
    "        if aab[i][0] in levels1[j]:\n",
    "            abcd = []\n",
    "            for l in aab[i]:\n",
    "                abcd.append(list(levels1[j]).index(l))\n",
    "            agg_structure1.append(abcd)\n",
    "\n",
    "\n",
    "'''Adding bottom level to the aggregation structure'''\n",
    "for i in range(len(bottom_level)):\n",
    "    agg_structure1.append([i])\n",
    "\n",
    "# %%\n",
    "#defining the tree for hierarchy \n",
    "ht = HierarchyTree.from_nodes(nodes=hierarchy, df=df1)\n",
    "#ht = HierarchyTree.from_nodes(nodes=hierarchy, df=df)\n",
    "sum_mat, sum_mat_labels = hts.functions.to_sum_mat(ht) #sum_mat_lables are the all the nodes in the hierarchy\n",
    "df2 = pd.DataFrame(columns = sum_mat_labels)\n",
    "for col in sum_mat_labels:\n",
    "    df2[col] = df1[col]\n",
    "   \n",
    "  \n",
    "df2 = df1.iloc[: , :len(agg_structure1)]\n",
    "df2['date'] =  df2.index\n",
    "df =df2               #df and df2 are equal in lenght \n",
    "    \n",
    "    \n",
    "#splitting data as train and test\n",
    "df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "df_test = df2[train_len: len(df2)] \n",
    "\n",
    "columns = df.columns\n",
    "df_list = []        # origignal data in list format \n",
    "for i in columns:\n",
    "    df_list.append(df2[i].to_list())\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'prediction_length': test_len,\n",
    "    'freq': freq\n",
    "}\n",
    "\n",
    "\n",
    "train_data = [{\"start\": df.index[0], \"target\": df[i].values[:-metadata['prediction_length']]} for i in df.columns]\n",
    "test_data = [{\"start\": df.index[0], \"target\": df[i].values} for i in df.columns]\n",
    "\n",
    "train_ds = ListDataset(\n",
    "    data_iter=train_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "\n",
    "test_ds = ListDataset(\n",
    "    data_iter=test_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata\n",
    "prediction_length = test_len\n",
    "context_length = test_len\n",
    "num_samples = 24\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, tss = get_lag_llama_predictions(\n",
    "    test_ds,\n",
    "    prediction_length=prediction_length,\n",
    "    num_samples=num_samples,\n",
    "    context_length=context_length,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "quantile = [0.1, 0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "evaluator = Evaluator(quantiles=quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # distr_output=\"neg_bin\",\n",
    "        # scaling=\"mean\",\n",
    "        nonnegative_pred_samples=True,\n",
    "        aug_prob=0,\n",
    "        lr=1e-2,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        # rope_scaling={\n",
    "        #     \"type\": \"linear\",\n",
    "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        # },\n",
    "\n",
    "        batch_size=32,\n",
    "        num_parallel_samples=num_samples,\n",
    "        trainer_kwargs = {\"max_epochs\": 150,}, # <- lightning trainer arguments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(train_ds, cache_data=True, shuffle_buffer_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts =  list(tqdm(forecast_it, desc=\"Forecasting batches\"))\n",
    "tss =  list(tqdm(ts_it,desc=\"Ground truth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantiles=quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_forecasts(forecasts):\n",
    "    samples1 = [forecast.samples for forecast in forecasts]\n",
    "    samples = np.transpose(samples1, (0, 2, 1))\n",
    "    samples = np.array(samples)\n",
    "    return samples\n",
    "\n",
    "# Usage example:\n",
    "samples = process_forecasts(forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = num_samples\n",
    "hist_i = a // metadata['prediction_length']\n",
    "\n",
    "past_m = []   \n",
    "past_std = []\n",
    "\n",
    "for i in range(hist_i):\n",
    "    try:\n",
    "        train_data_h = [{\"start\": df.index[0], \"target\": df[i1].values[:-a ]} for i1 in df.columns]\n",
    "        train_ds_h = ListDataset(data_iter=train_data_h, freq=metadata['freq'])\n",
    "        forecast_it_h, ts_it_h = make_evaluation_predictions(\n",
    "            dataset=train_ds_h,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples)\n",
    "\n",
    "        forecasts_h = list(tqdm(forecast_it_h, desc=\"Forecasting batches\"))\n",
    "        tss_h = list(tqdm(ts_it_h, desc=\"Ground truth\"))\n",
    "    \n",
    "        samples11 = []\n",
    "        for n in range(len(forecasts_h)):\n",
    "            samples11.append(forecasts_h[n].samples)\n",
    "            \n",
    "        samples1 = np.transpose(samples11, (0, 2, 1))\n",
    "        samples1 = samples1.tolist()\n",
    "        samples1 = np.array(samples1)\n",
    "\n",
    "        for k in range(len(samples1)):\n",
    "            aa = np.mean(samples1[k], axis=1)\n",
    "            ab = np.std(samples1[k], axis=1)\n",
    "            \n",
    "            if i <= 0:\n",
    "                past_m.append(aa)\n",
    "                past_std.append(ab)\n",
    "            else:\n",
    "                past_m[k] = np.hstack((past_m[k], aa))\n",
    "                past_std[k] = np.hstack((past_std[k], ab))\n",
    "\n",
    "        a = a - metadata['prediction_length']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle the error gracefully, log it, and possibly retry or exit the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining the standard residuals for the past data., taking mean and sd from past data samples \n",
    "\n",
    "     residuals = (actual-mean(predicted))/std(predicted)\n",
    "     \n",
    " We want to store the ranks of these residuals to use in revised bottom-up forecasting. '''\n",
    "res = []\n",
    "for i in range(len(samples)):\n",
    "    ab = []\n",
    "    for j in range(num_samples):\n",
    "        p = len(df[columns[0]])-num_samples-test_len\n",
    "        aa = ((df[columns[i]][p+j]-past_m[i][j]))/past_std[i][j]\n",
    "        ab.append(aa)\n",
    "    res.append(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We need sorted samples to re-order accoeding to the ranks of the residuals'''\n",
    "\n",
    "prob_forecasting = probabilistic_forecasting()\n",
    "\n",
    "sorted_samples = prob_forecasting.sample_sorting_1(samples) # sample_sorting function is defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ranks for the residuals\n",
    "ranks=[]\n",
    "for i in range(len(res)):\n",
    "    ranks.append(ss.rankdata(res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorderd_samples = prob_forecasting.sample_reordering_3(samples, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BU_len = sum_mat.shape[1]       #len of the bottom series \n",
    "agg_len = len(sum_mat_labels)-BU_len  #length of the aggregated series\n",
    "agg_samples_index = list(range(0, agg_len))  #indices of the aggregated series \n",
    "agg_series_ = columns[0:agg_len]           # names of the aggregated series \n",
    "\n",
    "bottom_series = prob_forecasting.getting_bottom_series(samples, sum_mat,sum_mat_labels)   #bottom level without reodering\n",
    "bottom_series_r = prob_forecasting.getting_bottom_series(reorderd_samples,sum_mat,sum_mat_labels) #bottom series with rank reodering\n",
    "\n",
    "aggre_series_numbers = agg_structure1     #aggregation structutre \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom series of original observation \n",
    "bottom_series_og = prob_forecasting.getting_bottom_series(df_list,sum_mat,sum_mat_labels) #function defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecat using bottom-up method \n",
    "bottom_up_samples_ = prob_forecasting.bottom_up_forecast(columns,bottom_series,bottom_level,aggre_series_numbers,levels) #bottom_up_forecast function defined in forecasting_funtions.py file\n",
    "bottom_up_samples_ = prob_forecasting.list_reversal(bottom_up_samples_) # list_reversal function defined in forecasting_funtions.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BU forecast with reodering, we reoder the forecasted samples according to the ranks of the past forecast residuals\n",
    "revised_samples_ = prob_forecasting.bottom_up_revised_forecast(columns,bottom_series_r,bottom_level,aggre_series_numbers,levels,ranks)\n",
    "revised_samples_ = prob_forecasting.list_reversal(revised_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom up forecast using outlier reordering technique\n",
    "bottom_up_forecast_h = prob_forecasting.bottom_up_forecast_hueristic(bottom_series, bottom_series_og,columns,bottom_level,aggre_series_numbers,levels,0.3)\n",
    "bottom_up_samples_h = prob_forecasting.list_reversal(bottom_up_forecast_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean forecast using all three methods \n",
    "mean_bottom_up_ = prob_forecasting.finding_mean(bottom_up_samples_)\n",
    "mean_revised_forecast_ = prob_forecasting.finding_mean(revised_samples_)\n",
    "mean_bottom_up_h = prob_forecasting.finding_mean(bottom_up_samples_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Period\n",
    "# Define the start date\n",
    "start_period = Period(df_test.index[0], freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "bottom_up_samples_ = np.transpose(bottom_up_samples_,(0,2,1))\n",
    "\n",
    "revised_samples_ = np.transpose(revised_samples_,(0,2,1))\n",
    "\n",
    "bottom_up_samples_h = np.transpose(bottom_up_samples_h,(0,2,1))\n",
    "\n",
    "# BU samples \n",
    "BU_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_]\n",
    "\n",
    "\n",
    "# revised samples\n",
    "r_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in revised_samples_]\n",
    "\n",
    "\n",
    "\n",
    "# HRA samples\n",
    "h_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_h]\n",
    "\n",
    "loss_df = pd.DataFrame(columns=['Method', 'mean_absolute_QuantileLoss', 'mean_wQuantileLoss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "new_row = pd.Series({'Method': 'Base_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, BU_forecast)\n",
    "new_row = pd.Series({'Method': 'BU_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, r_forecast)\n",
    "new_row = pd.Series({'Method': 'r_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, h_forecast)\n",
    "new_row = pd.Series({'Method': 'h_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
