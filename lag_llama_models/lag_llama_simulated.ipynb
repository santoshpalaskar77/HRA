{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import hts\n",
    "from hts.hierarchy import HierarchyTree\n",
    "\n",
    "from  forecasting_functions import probabilistic_forecasting\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.models import TCNModel,StatsForecastAutoARIMA,NHiTSModel\n",
    "import scipy.stats as ss\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import properscoring as ps\n",
    "\n",
    "# Fitting ARIMA model manually \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' pl follow steps from: https://github.com/time-series-foundation-models/lag-llama/blob/main/README.md \n",
    "to set the working directory '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /lag-llama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir  lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
    "\n",
    "        # linear positional encoding scaling\n",
    "        rope_scaling={\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        },\n",
    "\n",
    "        batch_size=batch_size,\n",
    "        num_parallel_samples=num_samples,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
    "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  We have a hierarchy with a total number of series n = 7, aggregated series\n",
    "r = 3 and bottom series m = 4. Each series at the bottom level is generated\n",
    "using the ARIMA processes with linear trend. Parameters p and q are selected randomly from {0,1,2}.\n",
    "Error terms for the bottom-level series are drawn from multivariate Gaussian. We consider\n",
    " the strong correlation between the series with the same parent and the moderate or small correlation\n",
    " between the series with different parents.\n",
    "\n",
    "We have folowing hieararchical structure\n",
    "           total   - level 0\n",
    "          /   \\\n",
    "         A     B   - level 1\n",
    "       / \\    / \\\n",
    "      AA AB BA BB  - level 2 or bottom level\n",
    "      \n",
    "      '''\n",
    "def make_arma(num):\n",
    "    '''Function generates the samples from ARIMA process\n",
    "    \n",
    "    parameter\n",
    "    -----------\n",
    "    num : number of sample points we want to generate\n",
    "    \n",
    "    '''\n",
    "    p_q_par = [0,1,2]\n",
    "    \n",
    "    # specify the ARIMA parameters\n",
    "    p = random.sample(p_q_par,1)[0]  # order of the autoregressive component\n",
    "    q = random.sample(p_q_par,1)[0]  # order of the moving average component\n",
    "    if p ==  0 and q == 1 :\n",
    "        arparams = np.array([0])\n",
    "        maparams = np.array([0.7])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "    elif p ==  1 and q == 0 :\n",
    "        arparams = np.array([0.4])\n",
    "        maparams = np.array([0])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "    elif p ==  1 and q == 1 :\n",
    "        arparams = np.array([0.4])\n",
    "        maparams = np.array([0.7])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "    elif p ==  2 and q == 1 :\n",
    "        arparams = np.array([0.4,0.2])\n",
    "        maparams = np.array([0.7])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "    elif p == 1 and q == 2 :\n",
    "        arparams = np.array([0.4])\n",
    "        maparams = np.array([0.7,0.3])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "    else  :\n",
    "        arparams = np.array([0.4,0.2])\n",
    "        maparams = np.array([0.7,0.3])\n",
    "        arparams = np.r_[1, -arparams]\n",
    "        maparams = np.r_[1, maparams]\n",
    "        y = arma_generate_sample(arparams, maparams, num)\n",
    "        return(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the length of the train, test and validation set \n",
    "# We can generate the time series of desired size using following parameters\n",
    "np.random.seed(1001)\n",
    "\n",
    "try:\n",
    "    test_len = int(input(\"Please enter the test length as an integer: \"))\n",
    "    print(f\"Test length is set to: {test_len}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a valid integer.\")\n",
    "\n",
    "train_len = 300\n",
    "total_len = train_len+test_len\n",
    "split_val = train_len\n",
    "\n",
    "\n",
    "\n",
    "''' We took the 110 samples from Arima process last 10 observation as test points'''\n",
    "AA = make_arma(total_len) \n",
    "AB = make_arma(total_len)\n",
    "BA = make_arma(total_len)\n",
    "BB = make_arma(total_len)\n",
    "\n",
    "\n",
    "'''Defining mean and covariance matrix to draw residuals from mutlivariate gaussian'''\n",
    "mean = [5,10,7,8]\n",
    "\n",
    "\n",
    "cov = [[2,0.8,0.5,0.2], [0.8, 3,0.22,0.23],[0.5,0.22,3,0.7],[0.2,0.23,0.7,3]]\n",
    "\n",
    "AA0,AB0,BA0,BB0  = np.random.multivariate_normal(mean, cov, total_len).T\n",
    "\n",
    "# Adding residuals to the samples \n",
    "AA = np.add(AA0,AA)\n",
    "AB = np.add(AB0,AB)\n",
    "BA = np.add(BA0,BA)\n",
    "BB = np.add(BB0,BB)\n",
    "\n",
    "# defining linear trend for each series\n",
    "\n",
    "x = np.linspace(0,3,total_len)\n",
    "\n",
    "AA1 = 10.5*x/2\n",
    "AB1 = 11*x/2\n",
    "BA1 = 13.5*x/2\n",
    "BB1 = 13*x/2\n",
    "# Adding trend to the series \n",
    "AA = np.add(AA1,AA)\n",
    "AB = np.add(AB1,AB)\n",
    "BA = np.add(BA1,BA)\n",
    "BB = np.add(BB1,BB)\n",
    "\n",
    "''' Defining aggregated series 'total, 'A', 'B' where aggregated series 'A' is the \n",
    "sum of series 'AA' and 'AB' and series 'B' is the sum of series 'BA' and 'BB' and the \n",
    "most aggregated series 'total' is the sum of 'A' and 'B'  '''\n",
    "A = np.add(AA,AB)\n",
    "B = np.add(BA,BB)\n",
    "total = np.add(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting hierarchical data \n",
    "plt.plot(total, label = 'Total') \n",
    "plt.plot(A, label = 'A')\n",
    "plt.plot(B, label = 'B')\n",
    "plt.plot(AA, label = 'AA')\n",
    "plt.plot(AB, label = 'AB')\n",
    "plt.plot(BA, label = 'BA')\n",
    "plt.plot(BB, label = 'BB')\n",
    "plt.legend()\n",
    "plt.title('Time series plot for hierarchical data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data frame and adding dates to the data for timse series forecast \n",
    "data = {'total': total,'A':A,'B':B, 'AA': AA, 'AB': AB, 'BA':BA,'BB':BB}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "date = pd.date_range(start='2022/1/1', periods=total_len)\n",
    "df.insert(0,'date',date)\n",
    "df = df.set_index('date')\n",
    "df11  = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' To use the Darts model, we need to transform the data into the form mentoned in their documentation.\n",
    " First, we create the levels according to the hierarchical structure and then \n",
    " define the hierarchy.    \n",
    " '''\n",
    "#levels and hierarchy will vary from data to data \n",
    "\n",
    "level0 = [0]        # level 0\n",
    "level1 = np.array(df.columns[1:3])  # level 1\n",
    "level1_r = list(range(1,3))    #stored the indices of the series \n",
    "level2 = np.array(df.columns[3:7])  # bottom level \n",
    "level2_r = list(range(3,7))                    \n",
    "levels = [level0,level1_r,level2_r]\n",
    "bottom_level = level2_r\n",
    "levels1 = [level1,level2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hierarchy\n",
    "\n",
    "total = {'total' : list(level1)}\n",
    "level1_h = {k: [v for v in level2  if v.startswith(k)] for k in level1}\n",
    "\n",
    "hierarchy = {**total,**level1_h}\n",
    "\n",
    "aac = list(hierarchy.keys())   # aggregated series \n",
    "aab= list(hierarchy.values())  # child nodes of each aggregated series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining aggregation structure that will be needed while geeting the aggregated\n",
    "forecast using child node forecasts'''\n",
    "agg_structure1 = []\n",
    "for i in range(len(aac)):\n",
    "    for j in range(len(levels1)):\n",
    "        if aab[i][0] in levels1[j]:\n",
    "            abcd = []\n",
    "            for l in aab[i]:\n",
    "                abcd.append(list(levels1[j]).index(l))\n",
    "            agg_structure1.append(abcd)\n",
    "\n",
    "\n",
    "'''Adding bottom level to the aggregation structure'''\n",
    "for i in range(len(bottom_level)):\n",
    "    agg_structure1.append([i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the tree for hierarchy \n",
    "ht = HierarchyTree.from_nodes(nodes=hierarchy, df=df)\n",
    "sum_mat, sum_mat_labels = hts.functions.to_sum_mat(ht) #sum_mat_lables are the all the nodes in the hierarchy\n",
    "df2 = pd.DataFrame(columns = sum_mat_labels)\n",
    "for col in sum_mat_labels:\n",
    "    df2[col] = df[col]\n",
    "   \n",
    "#defining the dataframe according to the hierarchy\n",
    "for col in sum_mat_labels:\n",
    "    df2[col] = df[col]\n",
    "    \n",
    "df2 = df.iloc[: , :len(agg_structure1)]\n",
    "df2['date'] =  df2.index\n",
    "df =df2\n",
    "df = df.drop(df.columns[-1], axis=1)\n",
    "df_test = df[train_len: len(df2)] \n",
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []        # origignal data in list format \n",
    "for i in columns:\n",
    "    df_list.append(df2[i].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'prediction_length': test_len,\n",
    "    'freq': '1D'\n",
    "}\n",
    "\n",
    "\n",
    "train_data = [{\"start\": df11.index[0], \"target\": df11[i].values[:-metadata['prediction_length']]} for i in df11.columns]\n",
    "test_data = [{\"start\": df11.index[0], \"target\": df11[i].values} for i in df11.columns]\n",
    "\n",
    "train_ds = ListDataset(\n",
    "    data_iter=train_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "\n",
    "test_ds = ListDataset(\n",
    "    data_iter=test_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.mx import SimpleFeedForwardEstimator, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata\n",
    "prediction_length = test_len\n",
    "context_length = prediction_length\n",
    "num_samples = 60\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, tss = get_lag_llama_predictions(\n",
    "    test_ds,\n",
    "    prediction_length=prediction_length,\n",
    "    num_samples=num_samples,\n",
    "    context_length=context_length,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "quantile = [0.1, 0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # distr_output=\"neg_bin\",\n",
    "        # scaling=\"mean\",\n",
    "        nonnegative_pred_samples=True,\n",
    "        aug_prob=0,\n",
    "        lr=5e-3,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        # rope_scaling={\n",
    "        #     \"type\": \"linear\",\n",
    "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        # },\n",
    "\n",
    "        batch_size=16,\n",
    "        num_parallel_samples=num_samples,\n",
    "        trainer_kwargs = {\"max_epochs\": 150,}, # <- lightning trainer arguments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(train_ds, cache_data=True, shuffle_buffer_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "\n",
    "\n",
    "forecasts =  list(tqdm(forecast_it, desc=\"Forecasting batches\"))\n",
    "tss =  list(tqdm(ts_it,desc=\"Ground truth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_forecasts(forecasts):\n",
    "    samples1 = [forecast.samples for forecast in forecasts]\n",
    "    samples = np.transpose(samples1, (0, 2, 1))\n",
    "    samples = np.array(samples)\n",
    "    return samples\n",
    "\n",
    "# Usage example:\n",
    "samples = process_forecasts(forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['prediction_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = num_samples\n",
    "hist_i = a // metadata['prediction_length']\n",
    "\n",
    "past_m = []   \n",
    "past_std = []\n",
    "\n",
    "for i in range(hist_i):\n",
    "    try:\n",
    "        train_data_h = [{\"start\": df11.index[0], \"target\": df11[i1].values[:-a ]} for i1 in df11.columns]\n",
    "        train_ds_h = ListDataset(data_iter=train_data_h, freq=metadata['freq'])\n",
    "        forecast_it_h, ts_it_h = make_evaluation_predictions(\n",
    "            dataset=train_ds_h,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples)\n",
    "\n",
    "        forecasts_h = list(tqdm(forecast_it_h, desc=\"Forecasting batches\"))\n",
    "        tss_h = list(tqdm(ts_it_h, desc=\"Ground truth\"))\n",
    "    \n",
    "        samples11 = []\n",
    "        for n in range(len(forecasts_h)):\n",
    "            samples11.append(forecasts_h[n].samples)\n",
    "            \n",
    "        samples1 = np.transpose(samples11, (0, 2, 1))\n",
    "        samples1 = samples1.tolist()\n",
    "        samples1 = np.array(samples1)\n",
    "\n",
    "        for k in range(len(samples1)):\n",
    "            aa = np.mean(samples1[k], axis=1)\n",
    "            ab = np.std(samples1[k], axis=1)\n",
    "            \n",
    "            if i <= 0:\n",
    "                past_m.append(aa)\n",
    "                past_std.append(ab)\n",
    "            else:\n",
    "                past_m[k] = np.hstack((past_m[k], aa))\n",
    "                past_std[k] = np.hstack((past_std[k], ab))\n",
    "\n",
    "        a = a - metadata['prediction_length']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle the error gracefully, log it, and possibly retry or exit the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining the standard residuals for the past data., taking mean and sd from past data samples \n",
    "\n",
    "     residuals = (actual-mean(predicted))/std(predicted)\n",
    "     \n",
    " We want to store the ranks of these residuals to use in revised bottom-up forecasting. '''\n",
    "res = []\n",
    "for i in range(len(samples)):\n",
    "    ab = []\n",
    "    for j in range(num_samples):\n",
    "        p = len(df[columns[0]])-num_samples-test_len\n",
    "        aa = ((df[columns[i]][p+j]-past_m[i][j]))/past_std[i][j]\n",
    "        ab.append(aa)\n",
    "    res.append(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We need sorted samples to re-order accoeding to the ranks of the residuals'''\n",
    "\n",
    "prob_forecasting = probabilistic_forecasting()\n",
    "\n",
    "sorted_samples = prob_forecasting.sample_sorting_1(samples) # sample_sorting function is defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ranks for the residuals\n",
    "ranks=[]\n",
    "for i in range(len(res)):\n",
    "    ranks.append(ss.rankdata(res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorderd_samples = prob_forecasting.sample_reordering_3(samples, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BU_len = sum_mat.shape[1]       #len of the bottom series \n",
    "agg_len = len(sum_mat_labels)-BU_len  #length of the aggregated series\n",
    "agg_samples_index = list(range(0, agg_len))  #indices of the aggregated series \n",
    "agg_series_ = columns[0:agg_len]           # names of the aggregated series \n",
    "\n",
    "bottom_series = prob_forecasting.getting_bottom_series(samples, sum_mat,sum_mat_labels)   #bottom level without reodering\n",
    "bottom_series_r = prob_forecasting.getting_bottom_series(reorderd_samples,sum_mat,sum_mat_labels) #bottom series with rank reodering\n",
    "\n",
    "aggre_series_numbers = agg_structure1     #aggregation structutre \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom series of original observation \n",
    "bottom_series_og = prob_forecasting.getting_bottom_series(df_list,sum_mat,sum_mat_labels) #function defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecat using bottom-up method \n",
    "bottom_up_samples_ = prob_forecasting.bottom_up_forecast(columns,bottom_series,bottom_level,aggre_series_numbers,levels) #bottom_up_forecast function defined in forecasting_funtions.py file\n",
    "bottom_up_samples_ = prob_forecasting.list_reversal(bottom_up_samples_) # list_reversal function defined in forecasting_funtions.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BU forecast with reodering, we reoder the forecasted samples according to the ranks of the past forecast residuals\n",
    "revised_samples_ = prob_forecasting.bottom_up_revised_forecast(columns,bottom_series_r,bottom_level,aggre_series_numbers,levels,ranks)\n",
    "revised_samples_ = prob_forecasting.list_reversal(revised_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom up forecast using outlier reordering technique\n",
    "bottom_up_forecast_h = prob_forecasting.bottom_up_forecast_hueristic(bottom_series, bottom_series_og,columns,bottom_level,aggre_series_numbers,levels,0.1)\n",
    "bottom_up_samples_h = prob_forecasting.list_reversal(bottom_up_forecast_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean forecast using all three methods \n",
    "mean_bottom_up_ = prob_forecasting.finding_mean(bottom_up_samples_)\n",
    "mean_revised_forecast_ = prob_forecasting.finding_mean(revised_samples_)\n",
    "mean_bottom_up_h = prob_forecasting.finding_mean(bottom_up_samples_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Period\n",
    "# Define the start date\n",
    "start_period = Period(df_test.index[0], 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "bottom_up_samples_ = np.transpose(bottom_up_samples_,(0,2,1))\n",
    "\n",
    "revised_samples_ = np.transpose(revised_samples_,(0,2,1))\n",
    "\n",
    "bottom_up_samples_h = np.transpose(bottom_up_samples_h,(0,2,1))\n",
    "\n",
    "# BU samples \n",
    "BU_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_]\n",
    "\n",
    "\n",
    "# revised samples\n",
    "r_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in revised_samples_]\n",
    "\n",
    "\n",
    "\n",
    "# HRA samples\n",
    "h_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_h]\n",
    "\n",
    "loss_df = pd.DataFrame(columns=['Method', 'mean_absolute_QuantileLoss', 'mean_wQuantileLoss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "new_row = pd.Series({'Method': 'Base_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, BU_forecast)\n",
    "new_row = pd.Series({'Method': 'BU_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, r_forecast)\n",
    "new_row = pd.Series({'Method': 'r_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, h_forecast)\n",
    "new_row = pd.Series({'Method': 'h_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbpf_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
