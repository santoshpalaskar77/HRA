{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import hts\n",
    "from hts.hierarchy import HierarchyTree\n",
    "\n",
    "from  forecasting_functions import probabilistic_forecasting\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.models import TCNModel,StatsForecastAutoARIMA,NHiTSModel\n",
    "import scipy.stats as ss\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import properscoring as ps\n",
    "\n",
    "# Fitting ARIMA model manually \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' pl follow steps from: https://github.com/time-series-foundation-models/lag-llama/blob/main/README.md \n",
    "to set the working directory '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /lag-llama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir  lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Labour.csv\")\n",
    "\n",
    "try:\n",
    "    test_len = int(input(\"Please enter the test length as an integer: \"))\n",
    "    print(f\"Test length is set to: {test_len}\")\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a valid integer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
    "\n",
    "        # linear positional encoding scaling\n",
    "        rope_scaling={\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        },\n",
    "\n",
    "        batch_size=batch_size,\n",
    "        num_parallel_samples=num_samples,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
    "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'Total':'total','Unnamed: 0':'date'}, inplace = True)\n",
    "\n",
    "columns1 = df.columns.to_list()\n",
    "columns1 = [element.replace(\"'\", \"\").strip(\"[]\").replace(',', '').replace(' ', '')  for element in columns1]\n",
    "\n",
    "columns1\n",
    "df.columns = columns1\n",
    "df.rename(columns = {'Total':'total'}, inplace = True)\n",
    "df =df.set_index('date')\n",
    "\n",
    "df11 =df\n",
    "# %%\n",
    "# Here we set the length of the train, test and validation set \n",
    "# We can generate the time series of desired size using following parameters\n",
    "#there are total 36 data points in each time series \n",
    "total_len = len(df)\n",
    "train_len =  total_len-test_len\n",
    "split_val = train_len\n",
    "freq = 'M'\n",
    "\n",
    "# %%\n",
    "\n",
    "''' To use the Darts model, we need to transform the data into the form mentoned in their documentation.\n",
    " First, we create the levels according to the hierarchical structure and then \n",
    " define the hierarchy.    \n",
    " '''\n",
    "#levels and hierarchy will vary from data to data \n",
    "\n",
    "level0 = [0]        # level 0\n",
    "level1 = np.array(df.columns[1:9])  # level 1\n",
    "level1_r = list(range(1,9))    \n",
    "level2 = np.array(df.columns[9:25])  \n",
    "level2_r = list(range(9,25))\n",
    "level3 = np.array(df.columns[25:len(df.columns)])\n",
    "level3_r = list(range(25,len(df.columns)))                     \n",
    "levels = [level0,level1_r,level2_r,level3_r]\n",
    "bottom_level = level3_r\n",
    "levels1 = [level1,level2,level3]\n",
    "\n",
    "agg_length= 25\n",
    "\n",
    "# %%\n",
    "# Defining hierarchy\n",
    "\n",
    "total = {'total' : list(level1)}\n",
    "level1_h = {k: [v for v in level2  if v.endswith(k)] for k in level1}\n",
    "level2_h = {k: [v for v in level3  if v.endswith(k)] for k in level2}\n",
    "\n",
    "\n",
    "hierarchy = {**total,**level1_h,**level2_h}\n",
    "\n",
    "aac = list(hierarchy.keys())   # aggregated series \n",
    "aab= list(hierarchy.values())  # child nodes of each aggregated series \n",
    "\n",
    "# %%\n",
    "level1\n",
    "\n",
    "# %%\n",
    "'''Defining aggregation structure that will be needed while geeting the aggregated\n",
    "forecast using child node forecasts'''\n",
    "agg_structure1 = []\n",
    "for i in range(len(aac)):\n",
    "    for j in range(len(levels1)):\n",
    "        if aab[i][0] in levels1[j]:\n",
    "            abcd = []\n",
    "            for l in aab[i]:\n",
    "                abcd.append(list(levels1[j]).index(l))\n",
    "            agg_structure1.append(abcd)\n",
    "\n",
    "\n",
    "'''Adding bottom level to the aggregation structure'''\n",
    "for i in range(len(bottom_level)):\n",
    "    agg_structure1.append([i])\n",
    "    \n",
    "\n",
    "# %%\n",
    "#defining the tree for hierarchy \n",
    "ht = HierarchyTree.from_nodes(nodes=hierarchy, df=df)\n",
    "sum_mat, sum_mat_labels = hts.functions.to_sum_mat(ht) #sum_mat_lables are the all the nodes in the hierarchy\n",
    "df2 = pd.DataFrame(columns = sum_mat_labels)\n",
    "\n",
    "#defining the dataframe according to the hierarchy\n",
    "for col in sum_mat_labels:\n",
    "    df2[col] = df[col]\n",
    "    \n",
    "df2 = df.iloc[: , :len(agg_structure1)]\n",
    "df2['date'] =  df2.index\n",
    "df =df2\n",
    "df = df.drop(df.columns[-1], axis=1)\n",
    "df_test = df[train_len: len(df2)] # remaining 10 points as test\n",
    "columns = df.columns\n",
    "\n",
    "df_list = []        # origignal data in list format \n",
    "for i in columns:\n",
    "    df_list.append(df2[i].to_list())\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'prediction_length': test_len,\n",
    "    'freq': '1M'\n",
    "}\n",
    "\n",
    "\n",
    "train_data = [{\"start\": df.index[0], \"target\": df[i].values[:-metadata['prediction_length']]} for i in df.columns]\n",
    "test_data = [{\"start\": df.index[0], \"target\": df[i].values} for i in df.columns]\n",
    "\n",
    "train_ds = ListDataset(\n",
    "    data_iter=train_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "\n",
    "test_ds = ListDataset(\n",
    "    data_iter=test_data,\n",
    "    freq=metadata['freq']\n",
    ")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata\n",
    "prediction_length = test_len\n",
    "context_length = test_len\n",
    "num_samples = 48\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, tss = get_lag_llama_predictions(\n",
    "    test_ds,\n",
    "    prediction_length=prediction_length,\n",
    "    num_samples=num_samples,\n",
    "    context_length=context_length,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "quantile = [0.1, 0.2, 0.3, 0.4,0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "evaluator = Evaluator(quantiles=quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # distr_output=\"neg_bin\",\n",
    "        # scaling=\"mean\",\n",
    "        nonnegative_pred_samples=True,\n",
    "        aug_prob=0,\n",
    "        lr=1e-2,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        # rope_scaling={\n",
    "        #     \"type\": \"linear\",\n",
    "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        # },\n",
    "\n",
    "        batch_size=32,\n",
    "        num_parallel_samples=num_samples,\n",
    "        trainer_kwargs = {\"max_epochs\": 150,}, # <- lightning trainer arguments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(train_ds, cache_data=True, shuffle_buffer_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts =  list(tqdm(forecast_it, desc=\"Forecasting batches\"))\n",
    "tss =  list(tqdm(ts_it,desc=\"Ground truth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantiles=quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_forecasts(forecasts):\n",
    "    samples1 = [forecast.samples for forecast in forecasts]\n",
    "    samples = np.transpose(samples1, (0, 2, 1))\n",
    "    samples = np.array(samples)\n",
    "    return samples\n",
    "\n",
    "# Usage example:\n",
    "samples = process_forecasts(forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = num_samples\n",
    "hist_i = a // metadata['prediction_length']\n",
    "\n",
    "past_m = []   \n",
    "past_std = []\n",
    "\n",
    "for i in range(hist_i):\n",
    "    try:\n",
    "        train_data_h = [{\"start\": df11.index[0], \"target\": df11[i1].values[:-a ]} for i1 in df11.columns]\n",
    "        train_ds_h = ListDataset(data_iter=train_data_h, freq=metadata['freq'])\n",
    "        forecast_it_h, ts_it_h = make_evaluation_predictions(\n",
    "            dataset=train_ds_h,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples)\n",
    "\n",
    "        forecasts_h = list(tqdm(forecast_it_h, desc=\"Forecasting batches\"))\n",
    "        tss_h = list(tqdm(ts_it_h, desc=\"Ground truth\"))\n",
    "    \n",
    "        samples11 = []\n",
    "        for n in range(len(forecasts_h)):\n",
    "            samples11.append(forecasts_h[n].samples)\n",
    "            \n",
    "        samples1 = np.transpose(samples11, (0, 2, 1))\n",
    "        samples1 = samples1.tolist()\n",
    "        samples1 = np.array(samples1)\n",
    "\n",
    "        for k in range(len(samples1)):\n",
    "            aa = np.mean(samples1[k], axis=1)\n",
    "            ab = np.std(samples1[k], axis=1)\n",
    "            \n",
    "            if i <= 0:\n",
    "                past_m.append(aa)\n",
    "                past_std.append(ab)\n",
    "            else:\n",
    "                past_m[k] = np.hstack((past_m[k], aa))\n",
    "                past_std[k] = np.hstack((past_std[k], ab))\n",
    "\n",
    "        a = a - metadata['prediction_length']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle the error gracefully, log it, and possibly retry or exit the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining the standard residuals for the past data., taking mean and sd from past data samples \n",
    "\n",
    "     residuals = (actual-mean(predicted))/std(predicted)\n",
    "     \n",
    " We want to store the ranks of these residuals to use in revised bottom-up forecasting. '''\n",
    "res = []\n",
    "for i in range(len(samples)):\n",
    "    ab = []\n",
    "    for j in range(num_samples):\n",
    "        p = len(df[columns[0]])-num_samples-test_len\n",
    "        aa = ((df[columns[i]][p+j]-past_m[i][j]))/past_std[i][j]\n",
    "        ab.append(aa)\n",
    "    res.append(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We need sorted samples to re-order accoeding to the ranks of the residuals'''\n",
    "\n",
    "prob_forecasting = probabilistic_forecasting()\n",
    "\n",
    "sorted_samples = prob_forecasting.sample_sorting_1(samples) # sample_sorting function is defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ranks for the residuals\n",
    "ranks=[]\n",
    "for i in range(len(res)):\n",
    "    ranks.append(ss.rankdata(res[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorderd_samples = prob_forecasting.sample_reordering_3(samples, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BU_len = sum_mat.shape[1]       #len of the bottom series \n",
    "agg_len = len(sum_mat_labels)-BU_len  #length of the aggregated series\n",
    "agg_samples_index = list(range(0, agg_len))  #indices of the aggregated series \n",
    "agg_series_ = columns[0:agg_len]           # names of the aggregated series \n",
    "\n",
    "bottom_series = prob_forecasting.getting_bottom_series(samples, sum_mat,sum_mat_labels)   #bottom level without reodering\n",
    "bottom_series_r = prob_forecasting.getting_bottom_series(reorderd_samples,sum_mat,sum_mat_labels) #bottom series with rank reodering\n",
    "\n",
    "aggre_series_numbers = agg_structure1     #aggregation structutre \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom series of original observation \n",
    "bottom_series_og = prob_forecasting.getting_bottom_series(df_list,sum_mat,sum_mat_labels) #function defined in forecasting_funtions.py file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecat using bottom-up method \n",
    "bottom_up_samples_ = prob_forecasting.bottom_up_forecast(columns,bottom_series,bottom_level,aggre_series_numbers,levels) #bottom_up_forecast function defined in forecasting_funtions.py file\n",
    "bottom_up_samples_ = prob_forecasting.list_reversal(bottom_up_samples_) # list_reversal function defined in forecasting_funtions.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BU forecast with reodering, we reoder the forecasted samples according to the ranks of the past forecast residuals\n",
    "revised_samples_ = prob_forecasting.bottom_up_revised_forecast(columns,bottom_series_r,bottom_level,aggre_series_numbers,levels,ranks)\n",
    "revised_samples_ = prob_forecasting.list_reversal(revised_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom up forecast using outlier reordering technique\n",
    "bottom_up_forecast_h = prob_forecasting.bottom_up_forecast_hueristic(bottom_series, bottom_series_og,columns,bottom_level,aggre_series_numbers,levels,0.3)\n",
    "bottom_up_samples_h = prob_forecasting.list_reversal(bottom_up_forecast_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean forecast using all three methods \n",
    "mean_bottom_up_ = prob_forecasting.finding_mean(bottom_up_samples_)\n",
    "mean_revised_forecast_ = prob_forecasting.finding_mean(revised_samples_)\n",
    "mean_bottom_up_h = prob_forecasting.finding_mean(bottom_up_samples_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Period\n",
    "# Define the start date\n",
    "start_period = Period(df_test.index[0], 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "bottom_up_samples_ = np.transpose(bottom_up_samples_,(0,2,1))\n",
    "\n",
    "revised_samples_ = np.transpose(revised_samples_,(0,2,1))\n",
    "\n",
    "bottom_up_samples_h = np.transpose(bottom_up_samples_h,(0,2,1))\n",
    "\n",
    "# BU samples \n",
    "BU_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_]\n",
    "\n",
    "\n",
    "# revised samples\n",
    "r_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in revised_samples_]\n",
    "\n",
    "\n",
    "\n",
    "# HRA samples\n",
    "h_forecast = [SampleForecast(samples=samples, start_date=start_period) for samples in bottom_up_samples_h]\n",
    "\n",
    "loss_df = pd.DataFrame(columns=['Method', 'mean_absolute_QuantileLoss', 'mean_wQuantileLoss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "new_row = pd.Series({'Method': 'Base_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, BU_forecast)\n",
    "new_row = pd.Series({'Method': 'BU_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, r_forecast)\n",
    "new_row = pd.Series({'Method': 'r_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantile)\n",
    "agg_metrics, item_metrics = evaluator(tss, h_forecast)\n",
    "new_row = pd.Series({'Method': 'h_forecast', 'mean_absolute_QuantileLoss': agg_metrics['mean_absolute_QuantileLoss'], 'mean_wQuantileLoss': agg_metrics['mean_wQuantileLoss']})\n",
    "loss_df.loc[len(loss_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
